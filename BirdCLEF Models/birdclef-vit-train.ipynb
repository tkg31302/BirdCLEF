{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"sourceType":"competition"},{"sourceId":11632994,"sourceType":"datasetVersion","datasetId":7298712}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport time\nimport logging\nfrom pathlib import Path\nfrom dataclasses import dataclass\nimport ast # For safely evaluating string representations of lists\n\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import StratifiedKFold, train_test_split # Added train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.optim import AdamW, Adam, SGD\nfrom torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau, StepLR, OneCycleLR\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm.auto import tqdm\nimport timm\nimport torchvision.transforms as T","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:40:56.121402Z","iopub.execute_input":"2025-05-18T22:40:56.121937Z","iopub.status.idle":"2025-05-18T22:40:56.126884Z","shell.execute_reply.started":"2025-05-18T22:40:56.121913Z","shell.execute_reply":"2025-05-18T22:40:56.126147Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"@dataclass\nclass Config:\n    train_csv: str = '/kaggle/input/birdclef-2025/train.csv'\n    taxonomy_csv: str = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    spectrogram_npy: str = '/kaggle/input/falcon-birdclef-cnn-preprocessed-dataset/falcon_birdclef_cnn_preprocessed_dataset.npy'\n    train_datadir: str = '/kaggle/input/birdclef-2025/train_audio'\n    LOAD_DATA: bool = True\n    \n    val_split_ratio: float = 0.2 # Ratio of data to use for validation (e.g., 0.2 for 20%)\n    \n    seed: int = 42\n    debug: bool = False\n    batch_size: int = 32 \n    num_workers: int = 0\n    epochs: int = 10 \n    device: str = 'cuda' if torch.cuda.is_available() else 'cpu'\n    \n    model_name: str = 'vit_base_patch16_224'\n    img_size: int = 224\n    pretrained: bool = True\n    in_channels: int = 1\n    \n    optimizer: str = 'AdamW'\n    lr: float = 1e-4 \n    scheduler: str = 'CosineAnnealingLR'\n    T_max: int = 10 \n    min_lr: float = 1e-6\n    weight_decay: float = 1e-5\n\n    criterion: str = 'BCEWithLogitsLoss'\n\ncfg = Config()\nif cfg.debug:\n    cfg.epochs = 2\n    # For debug, maybe use a smaller val_split_ratio if the dataset subset is very small\n    # cfg.val_split_ratio = 0.5 # Example for tiny debug dataset\ncfg.T_max = cfg.epochs\nlogging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:41:32.307756Z","iopub.execute_input":"2025-05-18T22:41:32.308113Z","iopub.status.idle":"2025-05-18T22:41:32.316117Z","shell.execute_reply.started":"2025-05-18T22:41:32.308084Z","shell.execute_reply":"2025-05-18T22:41:32.315301Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"def set_seed(seed: int = 42):\n    import random\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nset_seed(cfg.seed)\n\nif cfg.LOAD_DATA:\n    logging.info(\"Loading spectrograms...\")\n    spec_path = Path(cfg.spectrogram_npy)\n    if spec_path.exists():\n        spectrograms = np.load(spec_path, allow_pickle=True).item()\n        logging.info(f\"Loaded {len(spectrograms)} spectrograms from {spec_path}\")\n    else:\n        logging.error(f\"Spectrogram file not found: {spec_path}\")\n        spectrograms = {}\nelse:\n    logging.info(\"LOAD_DATA is False. Skipping spectrogram loading.\")\n    spectrograms = {}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:41:44.944085Z","iopub.execute_input":"2025-05-18T22:41:44.944686Z","iopub.status.idle":"2025-05-18T22:41:53.901503Z","shell.execute_reply.started":"2025-05-18T22:41:44.944666Z","shell.execute_reply":"2025-05-18T22:41:53.900800Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"class SpectrogramDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, cfg_obj: Config, specs: dict, mode: str = 'train'):\n        self.df = df.copy()\n        self.specs = specs\n        self.cfg = cfg_obj\n        self.mode = mode\n        self.missing_specs_count = 0\n\n        if 'sample_key' not in self.df.columns:\n            self.df['sample_key'] = (\n                self.df.filename\n                .str.replace('/', '_', regex=False)\n                .str.replace('.wav', '', regex=False)\n            )\n\n        try:\n            taxonomy = pd.read_csv(self.cfg.taxonomy_csv)\n        except FileNotFoundError:\n            logging.error(f\"Taxonomy CSV not found at {self.cfg.taxonomy_csv}.\")\n            self.label_to_idx = {}\n            self.num_classes = 0\n            raise\n\n        labels = taxonomy['primary_label'].unique().tolist()\n        self.label_to_idx = {lbl: idx for idx, lbl in enumerate(labels)}\n        self.num_classes = len(labels)\n        if self.num_classes == 0:\n            logging.warning(\"No classes found in taxonomy.\")\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx: int):\n        row = self.df.iloc[idx]\n        key = row['sample_key']\n        spec_data = self.specs.get(key)\n\n        if spec_data is None:\n            spec_data = np.zeros((self.cfg.in_channels, self.cfg.img_size, self.cfg.img_size), dtype=np.float32)\n            self.missing_specs_count +=1\n            if self.missing_specs_count < 10 or self.missing_specs_count % 100 == 0:\n                 logging.warning(f\"Spectrogram for key '{key}' not found. Using zero tensor. Total missing: {self.missing_specs_count}\")\n        else:\n            if spec_data.ndim == 2:\n                spec_data = np.expand_dims(spec_data, axis=0)\n\n        spec = torch.tensor(spec_data, dtype=torch.float32)\n\n        if spec.shape[1:] != (self.cfg.img_size, self.cfg.img_size):\n            spec = T.Resize([self.cfg.img_size, self.cfg.img_size], antialias=True)(spec)\n        \n        target = np.zeros(self.num_classes, dtype=np.float32)\n        primary_label = row['primary_label']\n        if primary_label in self.label_to_idx:\n            target[self.label_to_idx[primary_label]] = 1.0\n\n        sec_labels_str = row.get('secondary_labels', '[]')\n        if isinstance(sec_labels_str, str) and sec_labels_str and sec_labels_str != '[]':\n            try:\n                secondary_labels_list = ast.literal_eval(sec_labels_str)\n                for s_label in secondary_labels_list:\n                    if s_label in self.label_to_idx:\n                        target[self.label_to_idx[s_label]] = 1.0\n            except (ValueError, SyntaxError) as e:\n                logging.error(f\"Error parsing secondary_labels '{sec_labels_str}': {e}\")\n\n        target = torch.tensor(target, dtype=torch.float32)\n        return spec, target\n\ndef collate_specs(batch):\n    specs, targets = zip(*batch)\n    specs = torch.stack(specs)\n    targets = torch.stack(targets)\n    return specs, targets","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:41:59.140961Z","iopub.execute_input":"2025-05-18T22:41:59.141718Z","iopub.status.idle":"2025-05-18T22:41:59.152227Z","shell.execute_reply.started":"2025-05-18T22:41:59.141694Z","shell.execute_reply":"2025-05-18T22:41:59.151451Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"class CLEFClassifier(nn.Module):\n    def __init__(self, cfg_obj: Config, num_classes: int):\n        super().__init__()\n        self.cfg = cfg_obj\n        self.num_classes = num_classes\n\n        model_kwargs = {\n            'pretrained': self.cfg.pretrained,\n            'in_chans': self.cfg.in_channels,\n            'num_classes': 0\n        }\n        \n        if 'vit' in self.cfg.model_name.lower() or \\\n           'swin' in self.cfg.model_name.lower() or \\\n           'convnext' in self.cfg.model_name.lower():\n            model_kwargs['img_size'] = self.cfg.img_size\n\n        self.encoder = timm.create_model(\n            self.cfg.model_name,\n            **model_kwargs\n        )\n        \n        self.pool = nn.AdaptiveAvgPool2d(1) \n        feat_dim = self.encoder.num_features\n        self.head = nn.Linear(feat_dim, self.num_classes)\n\n    def forward(self, x):\n        feats = self.encoder(x) \n        if feats.dim() == 4:\n            feats = self.pool(feats)\n            feats = feats.view(feats.size(0), -1)\n        return self.head(feats)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:42:22.270069Z","iopub.execute_input":"2025-05-18T22:42:22.270721Z","iopub.status.idle":"2025-05-18T22:42:22.276524Z","shell.execute_reply.started":"2025-05-18T22:42:22.270696Z","shell.execute_reply":"2025-05-18T22:42:22.275942Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"def make_optimizer(model, cfg_obj: Config):\n    if cfg_obj.optimizer == 'AdamW':\n        return AdamW(model.parameters(), lr=cfg_obj.lr, weight_decay=cfg_obj.weight_decay)\n    elif cfg_obj.optimizer == 'Adam':\n        return Adam(model.parameters(), lr=cfg_obj.lr, weight_decay=cfg_obj.weight_decay)\n    elif cfg_obj.optimizer == 'SGD':\n        return SGD(model.parameters(), lr=cfg_obj.lr, weight_decay=cfg_obj.weight_decay, momentum=0.9)\n    else:\n        raise ValueError(f'Unknown optimizer: {cfg_obj.optimizer}')\n\ndef make_scheduler(opt, cfg_obj: Config, steps_per_epoch_for_onecycle: int = 0):\n    if cfg_obj.scheduler == 'CosineAnnealingLR':\n        return CosineAnnealingLR(opt, T_max=cfg_obj.T_max, eta_min=cfg_obj.min_lr)\n    elif cfg_obj.scheduler == 'ReduceLROnPlateau':\n        return ReduceLROnPlateau(opt, mode='max', factor=0.1, patience=2, min_lr=cfg_obj.min_lr, verbose=True)\n    elif cfg_obj.scheduler == 'StepLR':\n        return StepLR(opt, step_size=cfg_obj.epochs//3, gamma=0.1)\n    elif cfg_obj.scheduler == 'OneCycleLR':\n        if steps_per_epoch_for_onecycle == 0:\n            raise ValueError(\"steps_per_epoch_for_onecycle must be provided for OneCycleLR\")\n        return OneCycleLR(opt, max_lr=cfg_obj.lr,\n                          steps_per_epoch=steps_per_epoch_for_onecycle,\n                          epochs=cfg_obj.epochs,\n                          pct_start=0.3, div_factor=25, final_div_factor=1e4)\n    elif cfg_obj.scheduler is None or cfg_obj.scheduler.lower() == 'none':\n        return None\n    else:\n        raise ValueError(f'Unknown scheduler: {cfg_obj.scheduler}')\n\ndef make_loss(cfg_obj: Config):\n    if cfg_obj.criterion == 'BCEWithLogitsLoss':\n        return nn.BCEWithLogitsLoss()\n    else:\n        raise ValueError(f'Unknown criterion: {cfg_obj.criterion}')\n\ndef epoch_step(model, loader, opt, loss_fn, device, scheduler=None, train=True, cfg_obj: Config = None):\n    model.train() if train else model.eval()\n    epoch_losses, all_targets, all_predictions = [], [], []\n\n    progress_bar_desc = 'Train' if train else 'Valid'\n    if loader is None or len(loader) == 0: # Handle cases with no validation loader\n        if not train: # Only return if it's a validation step with no loader\n            return 0.0, 0.0 # Return dummy values for loss and AUC\n        # If it's a training step with no loader, something is wrong, but proceed cautiously\n        # Or raise an error: raise ValueError(\"Training loader cannot be None or empty\")\n\n    loader_pbar = tqdm(loader, desc=progress_bar_desc, leave=False) if loader else []\n\n\n    for batch in loader_pbar:\n        specs, targets_batch = batch\n        specs, targets_batch = specs.to(device), targets_batch.to(device)\n\n        if train:\n            opt.zero_grad()\n            preds = model(specs)\n            loss = loss_fn(preds, targets_batch)\n            loss.backward()\n            opt.step()\n            if scheduler and isinstance(scheduler, OneCycleLR):\n                scheduler.step()\n        else:\n            with torch.no_grad():\n                preds = model(specs)\n                loss = loss_fn(preds, targets_batch)\n\n        epoch_losses.append(loss.item())\n        all_targets.append(targets_batch.detach().cpu().numpy())\n        all_predictions.append(torch.sigmoid(preds).detach().cpu().numpy())\n        if hasattr(loader_pbar, 'set_postfix'): # Check if loader_pbar is a tqdm object\n             loader_pbar.set_postfix(loss=loss.item())\n    \n    if not epoch_losses: # If no batches were processed (empty loader)\n        return 0.0, 0.0\n\n\n    all_targets_np = np.vstack(all_targets)\n    all_predictions_np = np.vstack(all_predictions)\n    avg_epoch_loss = np.mean(epoch_losses)\n    class_auc_scores = []\n\n    for i in range(all_targets_np.shape[1]):\n        if np.sum(all_targets_np[:, i]) > 0 and len(np.unique(all_targets_np[:, i])) > 1:\n            try:\n                class_auc = roc_auc_score(all_targets_np[:, i], all_predictions_np[:, i])\n                class_auc_scores.append(class_auc)\n            except ValueError:\n                class_auc_scores.append(np.nan)\n        else:\n            class_auc_scores.append(np.nan)\n            \n    mean_auc = np.nanmean(class_auc_scores) if len(class_auc_scores) > 0 else 0.0\n    if np.isnan(mean_auc): mean_auc = 0.0\n    return avg_epoch_loss, mean_auc\n\ndef train_final_model(df: pd.DataFrame, cfg_obj: Config, loaded_spectrograms: dict):\n    logging.info(\"Starting training on a single train/validation split.\")\n\n    if 'primary_label' not in df.columns:\n        logging.error(\"Column 'primary_label' not found for train/val split stratification.\")\n        return None\n    if not Path(cfg_obj.taxonomy_csv).exists():\n        logging.error(f\"Taxonomy CSV {cfg_obj.taxonomy_csv} not found.\")\n        return None\n    try:\n        temp_taxonomy = pd.read_csv(cfg_obj.taxonomy_csv)\n        num_classes = len(temp_taxonomy['primary_label'].unique())\n        if num_classes == 0:\n            logging.error(\"No classes in taxonomy.\")\n            return None\n    except Exception as e:\n        logging.error(f\"Error reading taxonomy: {e}.\")\n        return None\n\n    # Create train/validation split\n    train_df, val_df = train_test_split(\n        df,\n        test_size=cfg_obj.val_split_ratio,\n        random_state=cfg_obj.seed,\n        stratify=df['primary_label'] if 'primary_label' in df else None # Stratify if possible\n    )\n    logging.info(f\"Training data: {len(train_df)} samples, Validation data: {len(val_df)} samples\")\n\n    train_ds = SpectrogramDataset(train_df, cfg_obj, loaded_spectrograms, 'train')\n    val_ds = SpectrogramDataset(val_df, cfg_obj, loaded_spectrograms, 'valid')\n\n    if train_ds.missing_specs_count > 0:\n        logging.warning(f\"Train DS: {train_ds.missing_specs_count}/{len(train_ds)} missing specs.\")\n    if val_ds.missing_specs_count > 0:\n        logging.warning(f\"Valid DS: {val_ds.missing_specs_count}/{len(val_ds)} missing specs.\")\n    if train_ds.num_classes == 0:\n        logging.error(\"Train Dataset has 0 classes. Aborting.\")\n        return None\n\n    tr_loader = DataLoader(train_ds, batch_size=cfg_obj.batch_size, shuffle=True, num_workers=cfg_obj.num_workers, collate_fn=collate_specs, pin_memory=(cfg_obj.device.startswith('cuda')), persistent_workers=(cfg_obj.num_workers > 0))\n    v_loader  = DataLoader(val_ds, batch_size=cfg_obj.batch_size * 2, shuffle=False, num_workers=cfg_obj.num_workers, collate_fn=collate_specs, pin_memory=(cfg_obj.device.startswith('cuda')), persistent_workers=(cfg_obj.num_workers > 0))\n\n    model = CLEFClassifier(cfg_obj, num_classes=train_ds.num_classes).to(cfg_obj.device)\n    \n    if torch.cuda.device_count() > 1 and cfg_obj.device.startswith('cuda'):\n        logging.info(f\"Using {torch.cuda.device_count()} GPUs via nn.DataParallel.\")\n        model = nn.DataParallel(model)\n\n    opt = make_optimizer(model, cfg_obj)\n    sch = make_scheduler(opt, cfg_obj, steps_per_epoch_for_onecycle=len(tr_loader))\n    loss_fn = make_loss(cfg_obj)\n    \n    best_val_auc = 0.0\n    best_epoch = -1\n    model_save_path = \"final_model_best_auc.pt\"\n    \n    epochs_pbar = tqdm(range(cfg_obj.epochs), desc=\"Training Epochs\")\n\n    for epoch in epochs_pbar:\n        train_loss, train_auc = epoch_step(model, tr_loader, opt, loss_fn, cfg_obj.device, scheduler=(sch if cfg_obj.scheduler == 'OneCycleLR' else None), train=True, cfg_obj=cfg_obj)\n        valid_loss, valid_auc = epoch_step(model, v_loader, None, loss_fn, cfg_obj.device, train=False, cfg_obj=cfg_obj)\n        epochs_pbar.set_postfix(TrainLoss=f\"{train_loss:.4f}\", TrainAUC=f\"{train_auc:.4f}\", ValidLoss=f\"{valid_loss:.4f}\", ValidAUC=f\"{valid_auc:.4f}\")\n\n        if sch and not isinstance(sch, OneCycleLR):\n            if isinstance(sch, ReduceLROnPlateau): sch.step(valid_auc)\n            else: sch.step()\n        \n        if valid_auc > best_val_auc:\n            best_val_auc = valid_auc\n            best_epoch = epoch + 1\n            current_model_state_dict = model.module.state_dict() if isinstance(model, nn.DataParallel) else model.state_dict()\n            torch.save(current_model_state_dict, model_save_path)\n            logging.info(f\"  Epoch {epoch+1}: New best Valid AUC: {valid_auc:.4f}. Model saved to {model_save_path}\")\n    \n    logging.info(f\"Training complete. Best Valid AUC: {best_val_auc:.4f} at epoch {best_epoch}\")\n    \n    best_model_info = {\n        'model_path': model_save_path,\n        'best_val_auc': best_val_auc,\n        'best_epoch': best_epoch\n    }\n    \n    del model, opt, sch, tr_loader, v_loader, train_ds, val_ds\n    if cfg_obj.device == 'cuda': torch.cuda.empty_cache()\n        \n    return best_model_info","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:42:46.128757Z","iopub.execute_input":"2025-05-18T22:42:46.129286Z","iopub.status.idle":"2025-05-18T22:42:46.150314Z","shell.execute_reply.started":"2025-05-18T22:42:46.129260Z","shell.execute_reply":"2025-05-18T22:42:46.149551Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"if __name__ == '__main__':\n    start_time = time.time()\n    logging.info(f\"Using device: {cfg.device}\")\n    if cfg.device.startswith(\"cuda\"):\n        logging.info(f\"CUDA available. Device count: {torch.cuda.device_count()}\")\n        for i in range(torch.cuda.device_count()):\n            logging.info(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n\n    try:\n        main_df = pd.read_csv(cfg.train_csv)\n        logging.info(f\"Loaded train_csv: {len(main_df)} rows.\")\n    except FileNotFoundError:\n        logging.error(f\"Train CSV {cfg.train_csv} not found.\")\n        main_df = None\n\n    if main_df is not None and not main_df.empty and (spectrograms if 'spectrograms' in globals() else True) :\n        if not spectrograms and cfg.LOAD_DATA:\n             logging.error(\"Spectrograms dictionary is empty despite LOAD_DATA=True. Aborting.\")\n             exit()\n\n        if cfg.debug:\n            logging.info(\"DEBUG mode: Using small subset.\")\n            # Ensure enough samples for train_test_split, especially if stratifying\n            min_samples_for_split = 2 \n            if 'primary_label' in main_df.columns:\n                 # Check if stratification is possible with debug sample size\n                 label_counts = main_df['primary_label'].value_counts()\n                 if any(label_counts < min_samples_for_split): # Need at least 2 samples of each class for stratification if val_split_ratio < 0.5\n                     logging.warning(\"DEBUG: Some classes have less than 2 samples. Stratification in train_test_split might be problematic or disabled.\")\n\n            current_len = len(main_df)\n            debug_sample_size = min(1000, current_len)\n            if current_len > debug_sample_size :\n                 # Simplified sampling for debug to avoid issues with very small classes for train_test_split\n                main_df = main_df.sample(n=debug_sample_size, random_state=cfg.seed, replace=False if current_len >= debug_sample_size else True).reset_index(drop=True)\n\n            logging.info(f\"Debug DataFrame size: {len(main_df)}.\")\n\n        # --- Call the new training function ---\n        trained_model_info = train_final_model(main_df, cfg, spectrograms)\n        # ------------------------------------\n\n        if trained_model_info and Path(trained_model_info['model_path']).exists():\n            logging.info(f\"\\n--- Final Model Preparation ---\")\n            best_model_path = trained_model_info['model_path']\n            logging.info(f\"Preparing final model from: {best_model_path}\")\n            \n            try:\n                taxonomy_df = pd.read_csv(cfg.taxonomy_csv)\n                num_classes_final = len(taxonomy_df['primary_label'].unique())\n            except Exception as e:\n                logging.error(f\"Cannot read taxonomy for final model: {e}\")\n                num_classes_final = 0\n\n            if num_classes_final > 0:\n                final_model_base = CLEFClassifier(cfg, num_classes=num_classes_final)\n                try:\n                    state_dict_to_load = torch.load(best_model_path, map_location='cpu')\n                    final_model_base.load_state_dict(state_dict_to_load)\n                    final_model_to_save = final_model_base.to(cfg.device) \n                    \n                    final_model_save_path = 'trained_model_final.pt' # New name for the final model\n                    torch.save(final_model_to_save.state_dict(), final_model_save_path)\n                    logging.info(f\"Successfully saved final trained model to {final_model_save_path} (from epoch {trained_model_info['best_epoch']}, Val AUC: {trained_model_info['best_val_auc']:.4f})\")\n                except Exception as e:\n                    logging.error(f\"Error loading/saving the final model: {e}\")\n            else:\n                logging.error(\"Cannot instantiate final model (num_classes=0).\")\n        else:\n            logging.warning(\"Training did not yield a saved model or model_info is missing.\")\n            \n    elif main_df is None or main_df.empty:\n        logging.error(\"Main DataFrame empty. Training aborted.\")\n    elif not spectrograms and cfg.LOAD_DATA :\n        logging.error(\"Spectrograms empty. Training aborted.\")\n        \n    end_time = time.time()\n    logging.info(f\"Total execution time: {(end_time - start_time)/60:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-18T22:42:57.761705Z","iopub.execute_input":"2025-05-18T22:42:57.762448Z","execution_failed":"2025-05-18T23:57:40.791Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Training Epochs:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"739b8200c7d147fa85b6a6756fea3aa4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/715 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/715 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Valid:   0%|          | 0/90 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Train:   0%|          | 0/715 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40446053c24b492a8bee4effc6597dab"}},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}