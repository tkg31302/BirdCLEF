{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":91844,"databundleVersionId":11361821,"isSourceIdPinned":false,"sourceType":"competition"},{"sourceId":414953,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":338623,"modelId":359638}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport gc\nimport warnings\nimport logging\nimport time\nimport math\nimport cv2\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport librosa\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport timm\nfrom tqdm.auto import tqdm\nimport torchvision\nwarnings.filterwarnings(\"ignore\")\nlogging.basicConfig(level=logging.ERROR)\n\nimport re\nfrom torch.quantization import quantize_dynamic","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.565490Z","iopub.execute_input":"2025-05-27T21:50:25.565797Z","iopub.status.idle":"2025-05-27T21:50:25.572746Z","shell.execute_reply.started":"2025-05-27T21:50:25.565777Z","shell.execute_reply":"2025-05-27T21:50:25.571365Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"class CFG:\n\n    N_FFT = 2048\n    HOP_LENGTH = 128\n    N_MELS = 512\n    FMIN = 20\n    FMAX = 16000\n    TARGET_SHAPE = (256,256)\n    FS = 32000  \n    WINDOW_SIZE = 5\n\n    model_path = '/kaggle/input/effnet_v2_focal/pytorch/default/1'\n    model_name = \"tf_efficientnetv2_s.in21k_ft_in1k\"\n    use_specific_folds = False\n    folds = [0,1,2,3]\n    in_channels = 1\n    device = 'cpu'  \n\n    # datasets\n    test_soundscapes = '/kaggle/input/birdclef-2025/test_soundscapes'\n    submission_csv = '/kaggle/input/birdclef-2025/sample_submission.csv'\n    taxonomy_csv = '/kaggle/input/birdclef-2025/taxonomy.csv'\n    \n    # Inference parameters\n    batch_size = 16\n    use_tta = False  \n    tta_count = 3\n    threshold = 0.5\n\n    # util\n    debug = False\n    debug_count = 3\n\ncfg = CFG()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.574464Z","iopub.execute_input":"2025-05-27T21:50:25.574831Z","iopub.status.idle":"2025-05-27T21:50:25.602113Z","shell.execute_reply.started":"2025-05-27T21:50:25.574809Z","shell.execute_reply":"2025-05-27T21:50:25.600876Z"}},"outputs":[],"execution_count":40},{"cell_type":"code","source":"print(f\"Using device: {cfg.device}\")\nprint(f\"Loading taxonomy data...\")\ntaxonomy_df = pd.read_csv(cfg.taxonomy_csv)\nspecies_ids = taxonomy_df['primary_label'].tolist()\nnum_classes = len(species_ids)\nprint(f\"Number of classes: {num_classes}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.603324Z","iopub.execute_input":"2025-05-27T21:50:25.603661Z","iopub.status.idle":"2025-05-27T21:50:25.634680Z","shell.execute_reply.started":"2025-05-27T21:50:25.603634Z","shell.execute_reply":"2025-05-27T21:50:25.633769Z"}},"outputs":[{"name":"stdout","text":"Using device: cpu\nLoading taxonomy data...\nNumber of classes: 206\n","output_type":"stream"}],"execution_count":41},{"cell_type":"code","source":"class BirdCLEFModel(nn.Module):\n    def __init__(self, cfg, num_classes):\n        super().__init__()\n        self.cfg = cfg\n        \n        self.backbone = timm.create_model(\n            cfg.model_name,\n            pretrained=False,  \n            in_chans=cfg.in_channels,\n            drop_rate=0.0,    \n            drop_path_rate=0.0\n        )\n        \n        backbone_out = self.backbone.classifier.in_features\n        self.backbone.classifier = nn.Identity()\n        self.pooling = nn.AdaptiveAvgPool2d(1)\n        self.feat_dim = backbone_out\n        self.classifier = nn.Linear(backbone_out, num_classes)\n        \n    def forward(self, x):\n        features = self.backbone(x)\n        if isinstance(features, dict):\n            features = features['features']\n        if len(features.shape) == 4:\n            features = self.pooling(features)\n            features = features.view(features.size(0), -1)\n\n        logits = self.classifier(features)\n        return logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.636821Z","iopub.execute_input":"2025-05-27T21:50:25.637088Z","iopub.status.idle":"2025-05-27T21:50:25.655749Z","shell.execute_reply.started":"2025-05-27T21:50:25.637067Z","shell.execute_reply":"2025-05-27T21:50:25.654759Z"}},"outputs":[],"execution_count":42},{"cell_type":"code","source":"def audio2melspec(audio_data, cfg):\n    \"\"\"Convert audio data to mel spectrogram\"\"\"\n    if np.isnan(audio_data).any():\n        mean_signal = np.nanmean(audio_data)\n        audio_data = np.nan_to_num(audio_data, nan=mean_signal)\n\n    mel_spec = librosa.feature.melspectrogram(\n        y=audio_data,\n        sr=cfg.FS,\n        n_fft=cfg.N_FFT,\n        hop_length=cfg.HOP_LENGTH,\n        n_mels=cfg.N_MELS,\n        fmin=cfg.FMIN,\n        fmax=cfg.FMAX,\n        power=2.0,\n        pad_mode=\"reflect\",\n        norm='slaney',\n        htk=True,\n        center=True,\n    )\n\n    mel_spec_db = librosa.power_to_db(mel_spec, ref=np.max)\n    mel_spec_norm = (mel_spec_db - mel_spec_db.min()) / (mel_spec_db.max() - mel_spec_db.min() + 1e-8)\n    \n    return mel_spec_norm\n\ndef process_audio_segment(audio_data, cfg):\n    \"\"\"Process audio segment to get mel spectrogram\"\"\"\n    if len(audio_data) < cfg.FS * cfg.WINDOW_SIZE:\n        audio_data = np.pad(audio_data, \n                          (0, cfg.FS * cfg.WINDOW_SIZE - len(audio_data)), \n                          mode='constant')\n    \n    mel_spec = audio2melspec(audio_data, cfg)\n    \n    if mel_spec.shape != cfg.TARGET_SHAPE:\n        mel_spec = cv2.resize(mel_spec, cfg.TARGET_SHAPE, interpolation=cv2.INTER_LINEAR)\n        \n    return mel_spec.astype(np.float32)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.656687Z","iopub.execute_input":"2025-05-27T21:50:25.656922Z","iopub.status.idle":"2025-05-27T21:50:25.683857Z","shell.execute_reply.started":"2025-05-27T21:50:25.656905Z","shell.execute_reply":"2025-05-27T21:50:25.683031Z"}},"outputs":[],"execution_count":43},{"cell_type":"code","source":"def find_model_files(cfg):\n    model_dir   = Path(cfg.model_path)\n    model_files = [str(p) for p in model_dir.glob(\"**/*.pt\")]\n    return model_files\n\n\ndef load_models(cfg, num_classes):\n\n    import re                              \n    from torch.quantization import quantize_dynamic\n\n    models      = []\n    model_files = find_model_files(cfg)\n\n    if not model_files:\n        print(f\"[WARN] No .pt found under {cfg.model_path}\")\n        return models\n\n    print(f\"Found {len(model_files)} model files.\")\n\n    # ---------- optional: only keep specified folds ----------\n    if cfg.use_specific_folds:\n        selected = []\n        for f in cfg.folds:\n            selected += [m for m in model_files if f\"fold{f}\" in m]\n        model_files = selected\n        print(f\"Using {len(model_files)} files for folds {cfg.folds}\")\n\n    # ---------- load each ckpt ----------\n    float_pat = re.compile(r\"(\\d+\\.\\d{2,4})\")      # <<< changed (两到四位小数)\n    for mp in model_files:\n        try:\n            print(f\"Loading {mp}\")\n            ckpt = torch.load(mp, map_location=torch.device(cfg.device))\n\n            net = BirdCLEFModel(cfg, num_classes)\n            if isinstance(ckpt, dict) and \"model_state_dict\" in ckpt:\n                net.load_state_dict(ckpt[\"model_state_dict\"])\n            else:\n                net.load_state_dict(ckpt, strict=False)\n            net   = net.to(cfg.device).eval()\n            print(f'net loaded')\n            \n            # ---- TorchScript + freeze ----\n            with torch.inference_mode():\n                dummy    = torch.rand(1, cfg.in_channels, *cfg.TARGET_SHAPE)\n                scripted = torch.jit.trace(net, dummy)\n                scripted = torch.jit.freeze(scripted)\n\n            # ---- fp16 dynamic quantisation (Linear layers) ----\n            scripted = quantize_dynamic(\n                scripted,\n                {torch.nn.Linear},\n                dtype=torch.float16                     # <<< changed\n            )\n\n            # ---- parse ckpt score as weight ----\n            m = float_pat.search(mp)\n            weight = float(m.group(1)) if m else 1.0   # fallback=1.0\n\n            models.append((scripted, weight))\n\n        except Exception as e:\n            print(f\"[ERR] {mp}: {e}\")\n\n    return models","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.684688Z","iopub.execute_input":"2025-05-27T21:50:25.684976Z","iopub.status.idle":"2025-05-27T21:50:25.711836Z","shell.execute_reply.started":"2025-05-27T21:50:25.684956Z","shell.execute_reply":"2025-05-27T21:50:25.710949Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"def predict_on_spectrogram(audio_path, models, cfg, species_ids):\n    \"\"\"\n    Inference for one .ogg; supports TTA & weighted ensemble.\n    Returns row_ids, predictions (list of np.array(num_classes))\n    \"\"\"\n    predictions, row_ids = [], []\n    soundscape_id        = Path(audio_path).stem\n\n    try:\n        print(f\"Processing {soundscape_id}\")\n        audio_data, _  = librosa.load(audio_path, sr=cfg.FS)\n        total_segments = len(audio_data) // (cfg.FS * cfg.WINDOW_SIZE)\n\n        # --------- take model weights once (np array) ---------\n        #w_arr = np.array([w for _, w in models], dtype=np.float32)   # <<< changed\n        w_arr = np.exp( np.array([w for _, w in models], np.float32) )\n        w_arr /= w_arr.sum()                                         # 归一化\n\n        for seg_idx in range(total_segments):\n            ss = seg_idx * cfg.FS * cfg.WINDOW_SIZE\n            es = ss +      cfg.FS * cfg.WINDOW_SIZE\n            segment_audio = audio_data[ss:es]\n\n            row_ids.append(f\"{soundscape_id}_{(seg_idx+1)*cfg.WINDOW_SIZE}\")\n\n            def _forward(mel):\n                mel = torch.tensor(mel, dtype=torch.float32).unsqueeze(0).unsqueeze(0\n                       ).to(cfg.device)\n                out_list = []\n                for mdl, _ in models:\n                    with torch.no_grad():\n                        out_list.append(mdl(mel).cpu().numpy().squeeze())   # ← 不再 sigmoid\n                return np.vstack(out_list)          # shape (N_models, C) logits\n\n            # ---------- make predictions ----------\n            if cfg.use_tta:\n                tta_stack = []\n                for tta_i in range(cfg.tta_count):\n                    mel = apply_tta(process_audio_segment(segment_audio, cfg),\n                                    tta_i)\n                    tta_stack.append(_forward(mel))\n                preds_raw = np.mean(tta_stack, axis=0)                 # (N, C)\n            else:\n                mel       = process_audio_segment(segment_audio, cfg)\n                preds_raw = _forward(mel)                              # (N, C)\n\n    # ---------- weighted logit average ----------\n            if preds_raw.shape[0] == 1:                  \n                final_logits = preds_raw[0]              \n            else:\n                scores  = np.array([w for _, w in models], dtype=np.float32)\n                exp_w   = np.exp(scores * 50)            \n                w_arr   = exp_w / exp_w.sum()            \n                final_logits = np.average(preds_raw, axis=0, weights=w_arr)\n            \n            final_preds = 1 / (1 + np.exp(-final_logits))\n            predictions.append(final_preds)\n\n    except Exception as e:\n        print(f\"[ERR] processing {audio_path}: {e}\")\n\n    return row_ids, predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.712944Z","iopub.execute_input":"2025-05-27T21:50:25.713237Z","iopub.status.idle":"2025-05-27T21:50:25.738878Z","shell.execute_reply.started":"2025-05-27T21:50:25.713217Z","shell.execute_reply":"2025-05-27T21:50:25.737792Z"}},"outputs":[],"execution_count":45},{"cell_type":"code","source":"def apply_tta(spec, tta_idx):\n    \"\"\"Apply test-time augmentation\"\"\"\n    if tta_idx == 0:\n        # Original spectrogram\n        return spec\n    elif tta_idx == 1:\n        # Time shift (horizontal flip)\n        return np.flip(spec, axis=1)\n    elif tta_idx == 2:\n        # Frequency shift (vertical flip)\n        return np.flip(spec, axis=0)\n    else:\n        return spec","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.740122Z","iopub.execute_input":"2025-05-27T21:50:25.740507Z","iopub.status.idle":"2025-05-27T21:50:25.765025Z","shell.execute_reply.started":"2025-05-27T21:50:25.740477Z","shell.execute_reply":"2025-05-27T21:50:25.763548Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"def run_inference(cfg, models, species_ids):\n    \"\"\"Run inference on all test soundscapes\"\"\"\n    test_files = list(Path(cfg.test_soundscapes).glob('*.ogg'))\n    \n    if cfg.debug:\n        print(f\"Debug mode enabled, using only {cfg.debug_count} files\")\n        test_files = test_files[:cfg.debug_count]\n    \n    print(f\"Found {len(test_files)} test soundscapes\")\n\n    all_row_ids = []\n    all_predictions = []\n\n    for audio_path in tqdm(test_files):\n        row_ids, predictions = predict_on_spectrogram(str(audio_path), models, cfg, species_ids)\n        all_row_ids.extend(row_ids)\n        all_predictions.extend(predictions)\n    \n    return all_row_ids, all_predictions","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.768198Z","iopub.execute_input":"2025-05-27T21:50:25.768533Z","iopub.status.idle":"2025-05-27T21:50:25.788264Z","shell.execute_reply.started":"2025-05-27T21:50:25.768511Z","shell.execute_reply":"2025-05-27T21:50:25.787241Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"def create_submission(row_ids, predictions, species_ids, cfg):\n    \"\"\"Create submission dataframe\"\"\"\n    print(\"Creating submission dataframe...\")\n\n    submission_dict = {'row_id': row_ids}\n    \n    for i, species in enumerate(species_ids):\n        submission_dict[species] = [pred[i] for pred in predictions]\n\n    submission_df = pd.DataFrame(submission_dict)\n    submission_df.set_index('row_id', inplace=True)\n    sample_sub = pd.read_csv(cfg.submission_csv, index_col='row_id')\n\n    missing_cols = set(sample_sub.columns) - set(submission_df.columns)\n    if missing_cols:\n        print(f\"Warning: Missing {len(missing_cols)} species columns in submission\")\n        for col in missing_cols:\n            submission_df[col] = 0.0\n\n    submission_df = submission_df[sample_sub.columns]\n    submission_df = submission_df.reset_index()\n    \n    return submission_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.789614Z","iopub.execute_input":"2025-05-27T21:50:25.790364Z","iopub.status.idle":"2025-05-27T21:50:25.811791Z","shell.execute_reply.started":"2025-05-27T21:50:25.790328Z","shell.execute_reply":"2025-05-27T21:50:25.810658Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"def main():\n    start_time = time.time()\n    print(\"Starting BirdCLEF-2025 inference...\")\n    print(f\"TTA enabled: {cfg.use_tta} (variations: {cfg.tta_count if cfg.use_tta else 0})\")\n\n    models = load_models(cfg, num_classes)\n    \n    if not models:\n        print(\"No models found! Please check model paths.\")\n        return\n    \n    print(f\"Model usage: {'Single model' if len(models) == 1 else f'Ensemble of {len(models)} models'}\")\n\n    row_ids, predictions = run_inference(cfg, models, species_ids)\n    submission_df = create_submission(row_ids, predictions, species_ids, cfg)\n    submission_path = 'submission.csv'\n    submission_df.to_csv(submission_path, index=False)\n    print(f\"Submission saved to {submission_path}\")\n    \n    end_time = time.time()\n    print(f\"Inference completed in {(end_time - start_time)/60:.2f} minutes\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.812929Z","iopub.execute_input":"2025-05-27T21:50:25.813218Z","iopub.status.idle":"2025-05-27T21:50:25.841965Z","shell.execute_reply.started":"2025-05-27T21:50:25.813183Z","shell.execute_reply":"2025-05-27T21:50:25.841021Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"if __name__ == \"__main__\":\n    main()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-27T21:50:25.842896Z","iopub.execute_input":"2025-05-27T21:50:25.843211Z","iopub.status.idle":"2025-05-27T21:50:40.642890Z","shell.execute_reply.started":"2025-05-27T21:50:25.843190Z","shell.execute_reply":"2025-05-27T21:50:40.641782Z"}},"outputs":[{"name":"stdout","text":"Starting BirdCLEF-2025 inference...\nTTA enabled: False (variations: 0)\nFound 2 model files.\nLoading /kaggle/input/effnet_v2_focal/pytorch/default/1/effnet_focal_best.pt\nnet loaded\nLoading /kaggle/input/effnet_v2_focal/pytorch/default/1/effnet_focal_final.pt\nnet loaded\nModel usage: Ensemble of 2 models\nFound 0 test soundscapes\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"0it [00:00, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5658eda768f8411089f7c448fb56d3b0"}},"metadata":{}},{"name":"stdout","text":"Creating submission dataframe...\nSubmission saved to submission.csv\nInference completed in 0.25 minutes\n","output_type":"stream"}],"execution_count":50},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}